{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We first import all necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, MinMaxScaler, MaxAbsScaler, Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#from sklearn.svm import SVC, LinearSVC\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier,\\\n",
    "BaggingClassifier\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, SelectFromModel, SelectPercentile, f_classif, chi2\n",
    "from sklearn.cross_validation import train_test_split, StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import log_loss#, f1_score, roc_auc_score, auc, make_scorer\n",
    "\n",
    "# Make matplotlib show our plots inline\n",
    "%matplotlib inline\n",
    "# Set figure size\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of train and test sets are: (878049, 9), (884262, 7)\n"
     ]
    }
   ],
   "source": [
    "# Get the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "print \"The dimensions of train and test sets are: {}, {}\".format(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 rows of train set: \n",
      "                 Dates        Category                      Descript  \\\n",
      "0  2015-05-13 23:53:00        WARRANTS                WARRANT ARREST   \n",
      "1  2015-05-13 23:53:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "2  2015-05-13 23:33:00  OTHER OFFENSES      TRAFFIC VIOLATION ARREST   \n",
      "3  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "4  2015-05-13 23:30:00   LARCENY/THEFT  GRAND THEFT FROM LOCKED AUTO   \n",
      "\n",
      "   DayOfWeek PdDistrict      Resolution                    Address  \\\n",
      "0  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "1  Wednesday   NORTHERN  ARREST, BOOKED         OAK ST / LAGUNA ST   \n",
      "2  Wednesday   NORTHERN  ARREST, BOOKED  VANNESS AV / GREENWICH ST   \n",
      "3  Wednesday   NORTHERN            NONE   1500 Block of LOMBARD ST   \n",
      "4  Wednesday       PARK            NONE  100 Block of BRODERICK ST   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.425892  37.774599  \n",
      "1 -122.425892  37.774599  \n",
      "2 -122.424363  37.800414  \n",
      "3 -122.426995  37.800873  \n",
      "4 -122.438738  37.771541  \n",
      "\n",
      "\n",
      "The first 5 rows of test set: \n",
      "   Id                Dates DayOfWeek PdDistrict                   Address  \\\n",
      "0   0  2015-05-10 23:59:00    Sunday    BAYVIEW   2000 Block of THOMAS AV   \n",
      "1   1  2015-05-10 23:51:00    Sunday    BAYVIEW        3RD ST / REVERE AV   \n",
      "2   2  2015-05-10 23:50:00    Sunday   NORTHERN    2000 Block of GOUGH ST   \n",
      "3   3  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
      "4   4  2015-05-10 23:45:00    Sunday  INGLESIDE  4700 Block of MISSION ST   \n",
      "\n",
      "            X          Y  \n",
      "0 -122.399588  37.735051  \n",
      "1 -122.391523  37.732432  \n",
      "2 -122.426002  37.792212  \n",
      "3 -122.437394  37.721412  \n",
      "4 -122.437394  37.721412  \n"
     ]
    }
   ],
   "source": [
    "# Check the first several rows\n",
    "print \"The first 5 rows of train set: \\n{}\".format(train.head())\n",
    "print \"\\n\"\n",
    "print \"The first 5 rows of test set: \\n{}\".format(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new dimension of the train set: (878049, 6)\n"
     ]
    }
   ],
   "source": [
    "# Drop the additional columns in train set\n",
    "train.drop(['Descript', 'Resolution', 'Address'], axis=1, inplace=True)\n",
    "print\"The new dimension of the train set: {}\".format(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate the target\n",
    "target = train['Category'].copy()\n",
    "train.drop(['Category'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate ID for the test set and drop the Address column\n",
    "test_id = test['Id'].copy()\n",
    "test.drop(['Id', 'Address'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function for preprocessing\n",
    "def preprocess(df):\n",
    "    df['Dates'] = pd.to_datetime(df['Dates']) #convert the Dates column to date type\n",
    "    df['hour'] = df.Dates.dt.hour #now create a new column contain only time\n",
    "    df.drop(['Dates'], axis=1, inplace=True) #drop the original column\n",
    "    df_new = pd.get_dummies(df) #convert categorical variables to numerical\n",
    "    print(\"Dataframe has been preprocessed\")\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe has been preprocessed\n",
      "Dataframe has been preprocessed\n"
     ]
    }
   ],
   "source": [
    "train = preprocess(train)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the target column to numeric as well\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=.2, random_state=88, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "The log_loss is: 2.61244429398\n",
      "The best estimator is:\n",
      "Pipeline(steps=[('robustscaler', RobustScaler(copy=True, with_centering=True, with_scaling=True)), ('lineardiscriminantanalysis', LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=1e-05))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   43.8s finished\n"
     ]
    }
   ],
   "source": [
    "# Remove all zero variance features\n",
    "selector_variance = VarianceThreshold()\n",
    "\n",
    "# Choose the scaler\n",
    "#scaler = StandardScaler() \n",
    "scaler = RobustScaler() \n",
    "#scaler = MinMaxScaler()\n",
    "#scaler = MaxAbsScaler()\n",
    "#scaler = Normalizer()\n",
    "\n",
    "# Choose the feature selection method\n",
    "#selector = SelectKBest()\n",
    "#selector = SelectPercentile()\n",
    "selector = SelectFromModel(ExtraTreesClassifier(random_state=88))\n",
    "\n",
    "# Choose the estimator\n",
    "#estimator = XGBClassifier()\n",
    "#estimator = GaussianNB() #13.3707515324\n",
    "#estimator = LogisticRegression() #2.60101362415\n",
    "estimator = LinearDiscriminantAnalysis() ###2.61244429398###\n",
    "#estimator = QuadraticDiscriminantAnalysis() #9.30360536618\n",
    "#estimator = RandomForestClassifier()\n",
    "#estimator = AdaBoostClassifier() #3.58991845559\n",
    "#estimator = GradientBoostingClassifier() # too long!\n",
    "#estimator = SGDClassifier() # doen't work!\n",
    "#estimator = LinearSVC()\n",
    "#estimator = KNeighborsClassifier() #17.6703306353\n",
    "#estimator = SVC()# too long!\n",
    "\n",
    "# Prepare the pipeline\n",
    "pipe = make_pipeline(#selector_variance,\n",
    "                     scaler,\n",
    "                     #PCA(),\n",
    "                     #selector,\n",
    "                     estimator)\n",
    "\n",
    "# Make the parameter grid\n",
    "params = {\n",
    "    #'pca__n_components': [num_pca],\n",
    "    #'selectkbest__k': [120, 130, 142],\n",
    "    #'selectkbest__score_func': [f_classif], \n",
    "    #'selectpercentile__score_func': [f_classif],\n",
    "    #'selectpercentile__percentile': [10, 20, 30, 50, 60],\n",
    "    #'selectfrommodel__threshold': ['mean', 'median'],\n",
    "    #'logisticregression__C': [1.5],\n",
    "    #'logisticregression__max_iter': [150],\n",
    "    #'logisticregression__multi_class': ['ovr'],\n",
    "    #'logisticregression__n_jobs': [-1],\n",
    "    #'logisticregression__random_state': [88],\n",
    "    #'logisticregression__solver': ['lbfgs'],\n",
    "    #'logisticregression__tol': [0.1],\n",
    "    'lineardiscriminantanalysis__solver': ['svd', 'lsqr'],\n",
    "    'lineardiscriminantanalysis__store_covariance': [False, True],\n",
    "    'lineardiscriminantanalysis__tol': [0.00001, 0.0001, 0.001],\n",
    "    #'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    #'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    #'lineardiscriminantanalysis__solver': ['svd', 'lsqr', 'eigen'],\n",
    "    #'sgdclassifier__loss': ['log'],\n",
    "    #'adaboostclassifier__n_estimators': [4],\n",
    "    #'adaboostclassifier__learning_rate': [0.02],\n",
    "    #'adaboostclassifier__random_state': [88],\n",
    "    #'randomforestclassifier__n_estimators': [10, 20],\n",
    "    #'randomforestclassifier__max_features': ['auto', 'sqrt'],\n",
    "    #'randomforestclassifier__n_jobs': [-1],\n",
    "    #'randomforestclassifier__random_state': [88], \n",
    "    #'adaboostclassifier__random_state': [88],\n",
    "    #'gradientboostingclassifier__random_state': [88]\n",
    "    #'xgbclassifier__nthread': [4],\n",
    "    #'xgbclassifier__objective': ['binary:logistic'],\n",
    "    #'xgbclassifier__learning_rate': [0.1], \n",
    "    #'xgbclassifier__reg_lambda': [1], \n",
    "    #'xgbclassifier__max_depth': [4],\n",
    "    #'xgbclassifier__min_child_weight': [4],\n",
    "    #'xgbclassifier__silent': [1],\n",
    "    #'xgbclassifier__subsample': [1],\n",
    "    #'xgbclassifier__colsample_bytree': [0.5],\n",
    "    #'xgbclassifier__scale_pos_weight': [1],\n",
    "    #'xgbclassifier__n_estimators': [100],\n",
    "    #'xgbclassifier__seed': [88]\n",
    "}\n",
    "\n",
    "# Make an StratifiedShuffleSplit iterator for cross-validation in GridSearchCV\n",
    "#sss = StratifiedShuffleSplit(y_train, n_iter=10, test_size=.3, random_state=88)\n",
    "\n",
    "# Make the model using GridSearchCV and run cross-validation\n",
    "clf = GridSearchCV(pipe,\n",
    "                   param_grid=params,\n",
    "                   scoring='log_loss',\n",
    "                   #cv=sss,\n",
    "                   n_jobs=-1,\n",
    "                   verbose=1)\n",
    "\n",
    "# Fit the model using premade clf\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict_proba(X_test)\n",
    "# Calculate the performance score\n",
    "score = log_loss(y_test, preds)\n",
    "print \"The log_loss is: {}\".format(score)\n",
    "print \"The best estimator is:\\n{}\".format(clf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the classes\n",
    "classes = clf.best_estimator_.classes_\n",
    "# Convert back to categories\n",
    "classes = le.inverse_transform(classes)\n",
    "# Store pedictions in a dataframe\n",
    "preds_test = clf.predict_proba(test)\n",
    "pred_df = pd.DataFrame(preds_test, index=test_id.values, columns=classes)\n",
    "# Combine the ID with the predictions\n",
    "out_df = pd.concat([test_id, pred_df], axis=1)\n",
    "# Save the output in a CSV file for submission\n",
    "out_df.to_csv(\"2.60100034896_LogisticRegression.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
